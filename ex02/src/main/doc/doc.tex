\documentclass[a4paper,headings=small]{scrartcl}
\KOMAoptions{DIV=12}

\usepackage[utf8x]{inputenc}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{multirow}
\usepackage{listings}

% define style of numbering
\numberwithin{equation}{section} % use separate numbering per section
\numberwithin{figure}{section}   % use separate numbering per section

% instead of using indents to denote a new paragraph, we add space before it
\setlength{\parindent}{0pt}
\setlength{\parskip}{10pt plus 1pt minus 1pt}

\title{Automatic Image Analysis - WS12/13 \\ Excercise 2 \\ \emph{Search, Seek and Categorize Images with OpenCV}}
\author{Team e: Marcus Grum (340733), Robin Vobruba (343773), \\ Robert Koppisch (214168), Nicolas Brieger (318599)}
\date{\today}

\pdfinfo{%
  /Title    (Automatic Image Analysis - WS12/13 - Excercise 2 - Search, Seek and Categorize Images with OpenCV)
  /Author   (Team e: Marcus Grum, Robin Vobruba, Robert Koppisch, Nicolas Brieger)
  /Creator  ()
  /Producer ()
  /Subject  ()
  /Keywords ()
}

% Simple picture reference
%   Usage: \image{#1}{#2}{#3}
%     #1: file-name of the image
%     #2: percentual width (decimal)
%     #3: caption/description
%
%   Example:
%     \image{myPicture}{0.8}{My huge house}
%     See fig. \ref{fig:myPicture}.
\newcommand{\image}[3]{
	\begin{figure}[htbp]
		\centering
		\includegraphics[width=#2\textwidth]{#1}
		\caption{#3}
		\label{fig:#1}
	\end{figure}
}

\newcommand{\generatedImgRoot}{../../../target}

\newcommand{\iBinThreshold}{98}
\newcommand{\iNumOfErosions}{2}
\newcommand{\dDetectionThreshold}{0.005}
\newcommand{\iFDNormDimensions}{64} %128/2

\begin{document}


\maketitle



\section{Object Recognition with OpenCV}


\subsection{Documentation}

The following homework deals with object recognition tasks using shape information.
For this, the Fourier Descriptors are used as it is described in the following:

An outline as mathematical function $f$ will be established from the shape of the objects.
\begin{align}
f=\begin{pmatrix} x_0 + i y_0 \\ x_1 + i y_1 \\ x_2 + i y_2 \\ ... \\ x_n + i y_n \end{pmatrix} 
\end{align}
It will be used for the object detection based on a functional comparison of two kinds of images:
The \emph{original} image containing the set of unclassified objects 
and the \emph{training} images that contain the objects to be classified separately.

The original picture that shall be analysed could be seen in fig. \ref{fig:\generatedImgRoot/pic_input}.

\image{\generatedImgRoot/pic_input}{1}{%
		Original picture (\emph{with unclassified objects}).}

The pictures of the database can be seen in fig. \ref{fig:\generatedImgRoot/pic_db1}.
and fig. \ref{fig:\generatedImgRoot/pic_db2}. Two types of leaves are to be
recognized and classified. The \emph{training} images are stored in an image database. 
This database can be seen as knowledge representation of the computer. 
In our implementation, they have been loaded in greyscale.

\image{\generatedImgRoot/pic_db1}{0.2}{%
		Training picture 1 (\emph{has to be classified}).}
\image{\generatedImgRoot/pic_db2}{0.5}{%
		Training picture 2 (\emph{has to be classified}).}

As a kind of preparation step, the training images have to be segmented such that in 
further processing steps the form of the objects can be extracted properly.
For this purpose, we use the function threshold(...) and a threshold of \iBinThreshold{} to binarize the images.
Then, the function erode(...) deletes small object connections like e.g. between the leaves.
For this, a threshold of \iNumOfErosions{} will be used.
Now, the contours can be extracted.

Representing the raw contour, the images looks like in fig. \ref{fig:\generatedImgRoot/pic_RawFourierDescriptor1} 
and in fig. \ref{fig:\generatedImgRoot/pic_RawFourierDescriptor2}.

\image{\generatedImgRoot/pic_RawFourierDescriptor1}{0.2}{%
		Raw picture 1 (\emph{contour with all frequencies}).}
\image{\generatedImgRoot/pic_RawFourierDescriptor2}{0.5}{%
		Raw picture 2 (\emph{contour with all frequencies}).}


The function $f$ stands for the contours of the object.
The contours of the object are represented as a vector of points, extracted from the training
images aswell as the original one. In order to reach an efficient, precise and robust representation of the contour, 
it will be represented with help of Fourier Descriptors. The fourier descriptor then is normalized as follows:

\begin{itemize}
\item Force translation invariance: The first element of the frequency vectors is set to 0.
\item Force scale invariance:  All elements of the frequencies vectors are divided by the 2nd element.
\item Force rotation invariance: The phase information of the frequencies are deleted.
\item Force less sensitivity to details: Therefore, the number of frequencies has been reduced.
In our implementation, we kept the \iFDNormDimensions{} low frequency components, while the rest was set to 0.
\end{itemize}

Thanks to the Fourier Descriptors and the mentioned modifications, 
the recognition algorithm is invariant to object translations,
rotations and scales.

Representing a contour using the DFT, the image looks like in fig. \ref{fig:\generatedImgRoot/pic_FourierDescriptor1} 
and in fig. \ref{fig:\generatedImgRoot/pic_FourierDescriptor2}.

\image{\generatedImgRoot/pic_FourierDescriptor1}{0.3}{%
		Reconstructed picture 1 (\emph{contours with reduced frequencies}).}
\image{\generatedImgRoot/pic_FourierDescriptor2}{0.5}{%
		Reconstructed picture 2 (\emph{contours with reduced frequencies}).}
\newpage
The final picture can be seen in fig. \ref{fig:\generatedImgRoot/pic_final}.
Due to the fact that we have implemented a live search engine,
objects that are currently analysed are marked in blue.
If the difference term of the classification is less than \dDetectionThreshold{},
the object will be categorized dependent on the types of leaves.
The first type of leaf has been colored in red 
and the second one has been colored in green.
If it was not categorized, the blue colore is kept to show that the object was considered.

\image{\generatedImgRoot/pic_final}{0.9}{%
		Original picture (\emph{with classified objects}).} 

Here can be seen, that the implementation of our algorithm works quite well.
The two types of leaves have been classified correctly.
\newpage


\section{Optional}

\subsection{What kind of problems do you encounter?}

Firstly, overlaps disturb the shape recognition.
For example when two leaves are overlapping,
it will be hard for the algorithm to apply the contour shapes.

Secondly, two different 3D objects may have similar 2D projections.
Those will be hard to distinguish,
e.g. the 2D projection of a ball is very similar to the 2D projection of a bottle from below.

Thirdly, objects that differ only in their texture, can not be recognized at all, 
because the algorithm only covers the object's shape.

\subsection{How could those problems be solved?}

For the overlap problem, one could install a gadget, that ensures that objects are separated properly.
This could be a fan that separates the objects with its wind. 
Additionally, one could prepare our algorithm in a way, so it uses certain overlaps of the 
contour as well or one could use just certain subparts of the contour for recognition.
The 3d problem could be solved in using a second camera that operates together with the first one.
The texture problem could be solved in using a texture recognition algorithm that cooperates with
our approach, independent from the question how it looks like.

\subsection{What kind of classification tasks should not be addressed with
this approach?}

Searching tasks can not be solved by the use of this approach because of possible overlaps
of the searching objects.
Control tasks like they will be used in newspaper productions to kick broken newspapers from
the assembly line can watch for damages in the newspaper because the observation is only in 2d,
but any further control tasks in 3d space will be much more difficult.
Material recognition tasks should not use this approach 
because a as a robot, that has to prepare different clothing materials, can not distinguish them.

\end{document}
