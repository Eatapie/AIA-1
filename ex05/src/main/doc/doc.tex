\documentclass[a4paper,headings=small]{scrartcl}
\KOMAoptions{DIV=12}

\usepackage[utf8x]{inputenc}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{multirow}
\usepackage{listings}
\usepackage{subfigure}

% define style of numbering
\numberwithin{equation}{section} % use separate numbering per section
\numberwithin{figure}{section}   % use separate numbering per section

% instead of using indents to denote a new paragraph, we add space before it
\setlength{\parindent}{0pt}
\setlength{\parskip}{10pt plus 1pt minus 1pt}

\title{Expectation Maximization}
\subtitle{Excercise 5 \\ Automatic Image Analysis - WS12/13}
\author{\textbf{Team E}: Marcus Grum (340733), Robin Vobruba (343773), \\ Robert Koppisch (214168), Nicolas Brieger (318599)\\Carola Thrams (314681)}
\date{\today}

\pdfinfo{%
	/Title    (Automatic Image Analysis - WS12/13 - Excercise 5 - Expectation Maximization)
	/Author   (Team E: Marcus Grum (340733), Robin Vobruba (343773), Robert Koppisch (214168), Nicolas Brieger (318599), Carola Thrams (314681))
	/Creator  ()
	/Producer ()
	/Subject  ()
	/Keywords ()
}

% Simple picture reference
%   Usage: \image{#1}{#2}{#3}
%     #1: file-name of the image
%     #2: percentual width (decimal)
%     #3: caption/description
%
%   Example:
%     \image{myPicture}{0.8}{My huge house}
%     See fig. \ref{fig:myPicture}.
\newcommand{\image}[3]{
	\begin{figure}[htbp]
		\centering
		\includegraphics[width=#2\textwidth]{#1}
		\caption{#3}
		\label{fig:#1}
	\end{figure}
}
\newcommand{\imgRoot}{../resources/img}
\newcommand{\imgGeneratedRoot}{../../../target}

\begin{document}

\maketitle

\section{Expectation Maximization - Object Recognition and Categorization with OpenCV}


\subsection{The Task}
In this exercise, object recognition and categorization
with help of Expectation Maximization (EM) is realized.
The task was, to detect a certain number of clusters in a 2 dimensional dataset,
based on the same number of samples per each of six different,
multivariate normal distributed clusters
(150 data samples per distribution),
such that they are visualized w.r.t. to their position in the corresponding dimensions.
The recognition itself is done again based on a multivariate normal distributions, 
that are trained through EM.

%\newpage
\section{Additional Documentation:}

Based on the very great documentation in the exercise slides,
in this section, only key aspects are shown that have been modified in our implementation.

Values $p( \vec x | \vec \mu, \vec \Sigma)$ are often very small.
In order to handle those, we used the log-likelyhood instead.
On two positions, it has been used in a simplified way in our implementation.
Firstly, it has been used in the function \lstinline!calcCompLogL()!
as follows.

The formula
\begin{align}
p( \vec x_i | \mu_j, \Sigma_j) =
    \frac{|\Sigma_j|^{-1/2}}{2 \pi^{d/2}}
    exp  \left( 
                     - \frac{1}{2} ( \vec x_i - \vec \mu_j )^{\top}
                       \vec \Sigma_j^{-1}
                       \frac{1}{2} ( \vec x_i - \vec \mu_j )
    \right)
\end{align}
has been implemented as follows:
\begin{align}
log \left( p( \vec x_i | \vec \mu_c, \vec \Sigma_c) \right) =
                     - \frac{1}{2} log \left( det(\vec \Sigma_c) \right)
                     - \frac{1}{2} ( \vec x_i - \vec \mu_c )^{\top}
                       \vec \Sigma_c^{-1}
                       \frac{1}{2} ( \vec x_i - \vec \mu_c )
                     - \frac{d}{2} log(2 \pi)
\end{align}

Secondly, the logarithms and a simplification have been used in the function \lstinline!calcMixtureLogL()!
as follows.

The formula
\begin{align}
log \left( \sum\nolimits_{j=1}^{N_c} \alpha_j p( \vec x_j | \theta_j )  \right) = 
      log \left( c \sum\nolimits_{j=1}^{N_c} \frac{1}{c} \alpha_j p( \vec x_j | \theta_j ) \right)
\end{align}
has been implemented as follows:
\begin{align}
... = log \left( c \right)
      + log \left(\sum\nolimits_{j=1}^{N_c} exp( log (\alpha_j)
         + log \left( p( \vec x_j | \theta_j ) \right) 
         - log \left( c \right) \right)
\end{align}
, where
\begin{align}
log(c) \approx max \left( log(\alpha_j) + log \left( p( \vec x_j | \theta_j ) \right) \right)
\end{align}


\section{Results:}

In Fig.~\ref{fig:clusterintResults},
you can see the results for an increasing number of clusters.
Each data sample is visualized by a blue point. 
The center of each cluster is marked with a red circle.
Their corresponding variances are visualized with help of an ellipse.

\begin{figure}
\subfigure[1 Cluster ]{\frame{\includegraphics[width=0.49\textwidth]{\imgGeneratedRoot/1_Cluster.png}}} \hfill
\subfigure[2 Clusters]{\frame{\includegraphics[width=0.49\textwidth]{\imgGeneratedRoot/1_Cluster.png}}} \hfill
\subfigure[3 Clusters]{\frame{\includegraphics[width=0.49\textwidth]{\imgGeneratedRoot/1_Cluster.png}}} \hfill
\subfigure[4 Clusters]{\frame{\includegraphics[width=0.49\textwidth]{\imgGeneratedRoot/1_Cluster.png}}} \hfill
\subfigure[5 Clusters]{\frame{\includegraphics[width=0.49\textwidth]{\imgGeneratedRoot/1_Cluster.png}}} \hfill
\subfigure[6 Clusters]{\frame{\includegraphics[width=0.49\textwidth]{\imgGeneratedRoot/1_Cluster.png}}}
\caption{result images of Categorization}
\label{fig:clusterintResults}
\end{figure}

\newpage
\section{Printed Code:}

\lstinputlisting[breaklines=true,language=C++]{../native/aia5.cpp}

\end{document}
